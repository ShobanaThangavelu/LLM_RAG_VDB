{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834e1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1da2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_store():\n",
    "    documents = load_documents()\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40351986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9beea8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[0]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53907451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OpenAIEmbeddings(openai_api_key=\"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\"), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "756b5a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590254b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 22 chunks.\n",
      "Open AI Chat with new GPT-3.5\n",
      "\n",
      "Pre-requisites\n",
      "\n",
      "Open AI Account\n",
      "\n",
      "http://chat.openai.com\n",
      "\n",
      "Get a developer key\n",
      "\n",
      "What is GPT 3.5?\n",
      "{'source': 'data\\\\chatopenai.md', 'start_index': 0}\n",
      "Saved 22 chunks to chroma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Generates Data Store \n",
    "generate_data_store()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3fe9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG implementation\n",
    "\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7053fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query text\n",
    "query_text = \" Which model is better model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58ed130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the DB.\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5bbb618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(metadata={'source': 'data\\\\chatopenai.md', 'start_index': 2042}, page_content='Get the best model from the AutoML run\\n\\nbest_model = automl_run.get_output()\\n\\nEvaluate the performance of the best model\\n\\nperf = best_model.predict(test.drop_columns(columns=[\"survived\"]))\\nprint(perf.auc())\\n```\\n\\nNow next to try Deep learning models\\n\\nSo here is my question\\n\\n```'), 0.6685426697032746), (Document(metadata={'source': 'data\\\\chatopenai.md', 'start_index': 315}, page_content='code-davinci-002 is a base model, so good for pure code-completion tasks\\ntext-davinci-002 is an InstructGPT model based on code-davinci-002\\ntext-davinci-003 is an improvement on text-davinci-002\\n```\\n\\nInformation available at https://beta.openai.com/docs/model-index-for-researchers'), 0.6529310657854615), (Document(metadata={'source': 'data\\\\chatopenai.md', 'start_index': 3011}, page_content='Evaluate the performance of the model\\n\\n_, acc = model.evaluate(x=test.drop(\"survived\", axis=1), y=test[\"survived\"])\\nprint(\"Test accuracy:\", acc)\\n```\\n\\nHere is another open source model called yolov5\\n\\nquestion - Can you create me a yolov5 deep learning code?\\n\\n```\\n\\nImport the necessary libraries'), 0.6261466095506409)]\n"
     ]
    }
   ],
   "source": [
    "# Search the DB.\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c302afab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "Get the best model from the AutoML run\n",
      "\n",
      "best_model = automl_run.get_output()\n",
      "\n",
      "Evaluate the performance of the best model\n",
      "\n",
      "perf = best_model.predict(test.drop_columns(columns=[\"survived\"]))\n",
      "print(perf.auc())\n",
      "```\n",
      "\n",
      "Now next to try Deep learning models\n",
      "\n",
      "So here is my question\n",
      "\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "code-davinci-002 is a base model, so good for pure code-completion tasks\n",
      "text-davinci-002 is an InstructGPT model based on code-davinci-002\n",
      "text-davinci-003 is an improvement on text-davinci-002\n",
      "```\n",
      "\n",
      "Information available at https://beta.openai.com/docs/model-index-for-researchers\n",
      "\n",
      "---\n",
      "\n",
      "Evaluate the performance of the model\n",
      "\n",
      "_, acc = model.evaluate(x=test.drop(\"survived\", axis=1), y=test[\"survived\"])\n",
      "print(\"Test accuracy:\", acc)\n",
      "```\n",
      "\n",
      "Here is another open source model called yolov5\n",
      "\n",
      "question - Can you create me a yolov5 deep learning code?\n",
      "\n",
      "```\n",
      "\n",
      "Import the necessary libraries\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context:  Which model is better model?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Augument the context\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42cb4e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Based on the provided context, the best model is the one obtained from the AutoML run. The code snippet evaluates the performance of the best model by using it to predict on the test data and then calculating the AUC score. Therefore, the best model from the AutoML run is considered superior in this context.\n",
      "Sources: ['data\\\\chatopenai.md', 'data\\\\chatopenai.md', 'data\\\\chatopenai.md']\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Generate the response\n",
    "   \n",
    "model = ChatOpenAI()\n",
    "response_text = model.predict(prompt)\n",
    "\n",
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "print(formatted_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
