{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9663ad9-217c-46d3-befc-8ccabeba890b",
   "metadata": {},
   "source": [
    "Document Splitting\n",
    "\n",
    "\n",
    "**Levels Of Text Splitting**\n",
    "* **Level 1: [Character Splitting](#CharacterSplitting)** - Simple static character chunks of data\n",
    "* **Level 2: [Recursive Character Text Splitting](#RecursiveCharacterSplitting)** - Recursive chunking based on a list of separators\n",
    "* **Level 3: [Document Specific Splitting](#DocumentSpecific)** - Various chunking methods for different document types (PDF, Python, Markdown)\n",
    "* **Level 4: [Semantic Splitting](#SemanticChunking)** - Embedding walk based chunking\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Level 1: Character Splitting <a id=\"CharacterSplitting\"></a>\n",
    "Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character sized chunks regardless of their content or form.\n",
    "\n",
    "This method isn't recommended for any applications - but it's a great starting point for us to understand the basics.\n",
    "\n",
    "* **Pros:** Easy & Simple\n",
    "* **Cons:** Very rigid and doesn't take into account the structure of your text\n",
    "\n",
    "Concepts to know:\n",
    "* **Chunk Size** - The number of characters you would like in your chunks. 50, 100, 100,000, etc.\n",
    "* **Chunk Overlap** - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks.\n",
    "\n",
    "Lets get some sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c96299fc-30f5-4edf-ac23-23a29f9c7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1cf67e-98d7-48bd-9867-f72be72e3f4a",
   "metadata": {},
   "source": [
    "Then let's split this text manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f11fb88f-17ed-44c2-b4de-a8a527fe63c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the text I would like to ch',\n",
       " 'unk up. It is the example text for ',\n",
       " 'this exercise']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list that will hold your chunks\n",
    "chunks = []\n",
    "\n",
    "chunk_size = 35 # Characters\n",
    "\n",
    "# Run through the a range with the length of your text and iterate every chunk_size you want\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140085b7-c6af-4003-923c-73feb1825965",
   "metadata": {},
   "source": [
    "\n",
    "When working with text in the language model world, we don't deal with raw strings. It is more common to work with documents. Documents are objects that hold the text you're concerned with, but also additional metadata which makes filtering and manipulation easier later.\n",
    "\n",
    "We could convert our list of strings into documents, but I'd rather start from scratch and create the docs.\n",
    "\n",
    "Let's load up LangChains `CharacterSplitter` to do this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d85945f0-4a09-4bd9-bdb6-bafe03089053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f3c0a-09c9-45a9-8f47-d28baf22b201",
   "metadata": {},
   "source": [
    "Then let's load up this text splitter. Requirement to specify `chunk overlap` and `separator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3dcbeb8d-c5a0-4047-8250-967313c20935",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='', strip_whitespace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae25bbe-d7d1-44da-820b-3cd34a1cfc67",
   "metadata": {},
   "source": [
    "Then we can actually split our text via `create_documents`. Note: `create_documents` expects a list of texts, so if you just have a string (like we do) you'll need to wrap it in `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afe4945b-ce08-49aa-a5dc-65a0e59922f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is the text I would like to ch'),\n",
       " Document(page_content='unk up. It is the example text for '),\n",
       " Document(page_content='this exercise')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331025f4-6ef4-4459-bcb6-df7824e78ce4",
   "metadata": {},
   "source": [
    "Notice how this time we have the same chunks, but they are in documents. These will play nicely with the rest of the LangChain world. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0f193-4098-4fb1-a42f-7f96cd182188",
   "metadata": {},
   "source": [
    "**Chunk Overlap & Separators**\n",
    "\n",
    "**Chunk overlap** will blend together our chunks so that the tail of Chunk #1 will overlap the head of Chunk #2 and so on and so forth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc66f496-7b0d-4b2a-a43d-e8f06d58c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=4, separator='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd5d7e36-b592-430e-9069-cc025c78d7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is the text I would like to ch'),\n",
       " Document(page_content='o chunk up. It is the example text'),\n",
       " Document(page_content='ext for this exercise')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4aaa8-b90b-499e-b2d5-bc623b5bb751",
   "metadata": {},
   "source": [
    "Notice how we have the same chunks, but now there is overlap between 1 & 2 and 2 & 3. The 'o ch' on the tail of Chunk #1 matches the 'o ch' of the head of Chunk #2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "814ce9aa-17c3-4205-b433-2eae612c2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='ch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb759b1f-dab0-4f5e-a0c0-220374313da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is the text I would like to'),\n",
       " Document(page_content='unk up. It is the example text for this exercise')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539aa1a2-67b8-4585-b0d3-306703ea856b",
   "metadata": {},
   "source": [
    "\n",
    "Basic Character splitting is likely only useful for a few applications.\n",
    "\n",
    "## Level 2: Recursive Character Text Splitting\n",
    "<a id=\"RecursiveCharacterSplitting\"></a>\n",
    "Let's jump a level of complexity.\n",
    "\n",
    "The problem with Level #1 is that we don't take into account the structure of our document at all. We simply split by a fix number of characters.\n",
    "\n",
    "The Recursive Character Text Splitter helps with this. With it, we'll specify a series of separatators which will be used to split our docs.\n",
    "\n",
    "You can see the default separators for LangChain [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L842). Let's take a look at them one by one.\n",
    "\n",
    "* \"\\n\\n\" - Double new line, or most commonly paragraph breaks\n",
    "* \"\\n\" - New lines\n",
    "* \" \" - Spaces\n",
    "* \"\" - Characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49f42bea-3d06-404d-9f8c-f15f7ff7591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f79f4-769b-474b-8d7d-19cb48407cd6",
   "metadata": {},
   "source": [
    "Then let's load up a larger piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0772695d-0c5e-4e19-bb69-14e9bd7a15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb158c-6bbe-4f49-95df-a8b43965a566",
   "metadata": {},
   "source": [
    "Now let's make our text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03ec54c4-bda6-4254-97dd-983775b1d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 65, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "887c7676-1e67-4084-94d3-59689eb399c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"One of the most important things I didn't understand about the\"),\n",
       " Document(page_content='world when I was a child is the degree to which the returns for'),\n",
       " Document(page_content='performance are superlinear.'),\n",
       " Document(page_content='Teachers and coaches implicitly told us the returns were linear.'),\n",
       " Document(page_content='\"You get out,\" I heard a thousand times, \"what you put in.\" They'),\n",
       " Document(page_content='meant well, but this is rarely true. If your product is only'),\n",
       " Document(page_content=\"half as good as your competitor's, you don't get half as many\"),\n",
       " Document(page_content='customers. You get no customers, and you go out of business.'),\n",
       " Document(page_content=\"It's obviously true that the returns for performance are\"),\n",
       " Document(page_content='superlinear in business. Some think this is a flaw of'),\n",
       " Document(page_content='capitalism, and that if we changed the rules it would stop being'),\n",
       " Document(page_content='true. But superlinear returns for performance are a feature of'),\n",
       " Document(page_content=\"the world, not an artifact of rules we've invented. We see the\"),\n",
       " Document(page_content='same pattern in fame, power, military victories, knowledge, and'),\n",
       " Document(page_content='even benefit to humanity. In all of these, the rich get richer.'),\n",
       " Document(page_content='[1]')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa00043-1655-4113-bb28-f3a998d5713a",
   "metadata": {},
   "source": [
    "Notice how now there are more chunks that end with a period \".\". This is because those likely are the end of a paragraph and the splitter first looks for double new lines (paragraph break).\n",
    "\n",
    "Once paragraphs are split, then it looks at the chunk size, if a chunk is too big, then it'll split by the next separator. If the chunk is still too big, then it'll move onto the next one and so forth.\n",
    "\n",
    "For text of this size, let's split on something bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6da8734e-47da-4a08-8459-9bf8bfed7fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\"),\n",
       " Document(page_content='Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor\\'s, you don\\'t get half as many customers. You get no customers, and you go out of business.'),\n",
       " Document(page_content=\"It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\")]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 450, chunk_overlap=0)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e99768f-8732-44e4-b8d8-cc5ac1fe4661",
   "metadata": {},
   "source": [
    "For this text, 450 splits the paragraphs perfectly. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f32a73-0c8a-498c-a3a1-3e7dba4658c9",
   "metadata": {},
   "source": [
    "## Level 3: Document Specific Splitting <a id=\"DocumentSpecific\"></a>\n",
    "\n",
    "let's start to handle document types other than normal prose in a .txt. What if you have pictures? or code snippets?\n",
    "\n",
    "Our first two levels wouldn't work great for this so we'll need to find a different tactic.\n",
    "\n",
    "This level is all about making your chunking strategy fit your different data formats. Let's run through a bunch of examples of this in action\n",
    "\n",
    "The Markdown, Python, and JS splitters will basically be similar to Recursive Character, but with different separators.\n",
    "\n",
    "### Markdown\n",
    "\n",
    "You can see the separators [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1175).\n",
    "\n",
    "Separators:\n",
    "* `\\n#{1,6}` - Split by new lines followed by a header (H1 through H6)\n",
    "* ```` ```\\n ```` - Code blocks\n",
    "* `\\n\\\\*\\\\*\\\\*+\\n` - Horizontal Lines\n",
    "* `\\n---+\\n` - Horizontal Lines\n",
    "* `\\n___+\\n` - Horizontal Lines\n",
    "* `\\n\\n` Double new lines\n",
    "* `\\n` - New line\n",
    "* `\" \"` - Spaces\n",
    "* `\"\"` - Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "298fe868-0872-4fa9-9146-fa33e9dd5706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1d390ed-d046-44f9-a492-9760141f7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ba14168-451b-4e9c-b1d0-d1eac6996ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# Fun in California\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15dcf8de-551a-4477-8e68-57c4c50ddbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Fun in California\\n\\n## Driving'),\n",
       " Document(page_content='Try driving on the 1 down to San Diego'),\n",
       " Document(page_content='### Food'),\n",
       " Document(page_content=\"Make sure to eat a burrito while you're\"),\n",
       " Document(page_content='there'),\n",
       " Document(page_content='## Hiking\\n\\nGo to Yosemite')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.create_documents([markdown_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56591620-ef0c-41c2-b539-35ad676ed20f",
   "metadata": {},
   "source": [
    "Notice how the splits gravitate towards markdown sections. However, it's still not perfect. Check out how there is a chunk with just \"there\" in it. You'll run into this at low-sized chunks.\n",
    "\n",
    "### Python\n",
    "\n",
    "See the python splitters [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1069)\n",
    "\n",
    "* `\\nclass` - Classes first\n",
    "* `\\ndef` - Functions next\n",
    "* `\\n\\tdef` - Indented functions\n",
    "* `\\n\\n` - Double New lines\n",
    "* `\\n` - New Lines\n",
    "* `\" \"` - Spaces\n",
    "* `\"\"` - Characters\n",
    "\n",
    "\n",
    "Let's load up our splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66edcde5-1e96-4b61-8636-8129d31d7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2afa8f93-6b07-484f-86ff-9836f5a5fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_text = \"\"\"\n",
    "class Person:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "for i in range(10):\n",
    "    print (i)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e8fcc85-714d-4b5c-a5ce-a3f30cfb447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a7b6dd89-6bb9-496a-a85d-3f1871ff9cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='class Person:\\n  def __init__(self, name, age):\\n    self.name = name\\n    self.age = age'),\n",
       " Document(page_content='p1 = Person(\"John\", 36)\\n\\nfor i in range(10):\\n    print (i)')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_splitter.create_documents([python_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c004f19-9e67-451e-abdd-b103acce2996",
   "metadata": {},
   "source": [
    "Check out how the class stays together in a single document, then the rest of the code is in a second document.\n",
    "\n",
    "### JS\n",
    "\n",
    "Very similar to python. See the separators [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L983).\n",
    "\n",
    "Separators:\n",
    "* `\\nfunction` - Indicates the beginning of a function declaration\n",
    "* `\\nconst` - Used for declaring constant variables\n",
    "* `\\nlet` - Used for declaring block-scoped variables\n",
    "* `\\nvar` - Used for declaring a variable\n",
    "* `\\nclass` - Indicates the start of a class definition\n",
    "* `\\nif` - Indicates the beginning of an if statement\n",
    "* `\\nfor` - Used for for-loops\n",
    "* `\\nwhile` - Used for while-loops\n",
    "* `\\nswitch` - Used for switch statements\n",
    "* `\\ncase` - Used within switch statements\n",
    "* `\\ndefault` - Also used within switch statements\n",
    "* `\\n\\n` - Indicates a larger separation in text or code\n",
    "* `\\n` - Separates lines of code or text\n",
    "* `\" \"` - Separates words or tokens in the code\n",
    "* `\"\"` - Makes every character a separate element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5225b66-4d79-455b-92a1-841fa23ccc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d75090fa-4d22-4348-8452-eb50eafa784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "javascript_text = \"\"\"\n",
    "// Function is called, the return value will end up in x\n",
    "let x = myFunction(4, 3);\n",
    "\n",
    "function myFunction(a, b) {\n",
    "// Function returns the product of a and b\n",
    "  return a * b;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "909fde28-43ba-4f07-b9ae-04c21db04055",
   "metadata": {},
   "outputs": [],
   "source": [
    "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, chunk_size=65, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b70d936-bc31-4ecc-b190-6dd8fffdacb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='// Function is called, the return value will end up in x'),\n",
       " Document(page_content='let x = myFunction(4, 3);'),\n",
       " Document(page_content='function myFunction(a, b) {'),\n",
       " Document(page_content='// Function returns the product of a and b\\n  return a * b;\\n}')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_splitter.create_documents([javascript_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc40dacd-09a0-4ce0-ae8c-87a3910a1408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7b48f-7da2-430f-a0dd-c8e1766854a3",
   "metadata": {},
   "source": [
    "We'll be using gpt-4o today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "59b1b6d2-4d84-41dc-8698-8be52e6f5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea28491-e225-4667-8666-3b0541dbf2b7",
   "metadata": {},
   "source": [
    "creating a quick helper function to convert the image from file to base64 so we can pass it to GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed1f096a-abd1-4380-af24-6c65074d2420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert image to base64\n",
    "def image_to_base64(image_path):\n",
    "    with Image.open(image_path) as image:\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=image.format)\n",
    "        img_str = base64.b64encode(buffered.getvalue())\n",
    "        return img_str.decode('utf-8')\n",
    "\n",
    "image_str = image_to_base64(\"design.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80becf02-83bd-4560-af3a-dece72259296",
   "metadata": {},
   "source": [
    "Then we can go ahead and pass our image to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "638d7f95-d181-4b4a-aeb0-b40367f0f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-4o\",\n",
    "                  max_tokens=1024)\n",
    "\n",
    "msg = chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\" : \"Please give a summary of the image provided. Be descriptive\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_str}\"\n",
    "                    },\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67130242-5d72-4304-a705-c9177558a7d4",
   "metadata": {},
   "source": [
    "Then the summary returned is what we will put into our vectordata base. Then when it comes time to do our retrieval process, we'll use these embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a51c2833-ee05-43b2-9cd2-6710d6e73ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The image is a detailed diagram illustrating the offerings of OpenAI integrated with Microsoft Azure. It showcases the structure and workflow of various AI services provided by OpenAI, specifically focusing on text, image, and file handling capabilities.\\n\\n### Main Components:\\n1. **API Gateway and Core AI Engine:**\\n   - The diagram begins with an API Gateway that manages API key authentication and provides access to OpenAI’s core models and engines.\\n   - The Core AI Engine interacts with various models.\\n\\n2. **Models and Engines:**\\n   - Models are managed through API keys and organization permissions.\\n   - The engines component, though marked as deprecated, links to these models.\\n\\n3. **Services and Capabilities:**\\n   - **Text:**\\n     - Generative text/code AI services using models like GPT-3 and Codex.\\n     - Capabilities include creating completions, edits, embeddings, and moderations.\\n   - **Fine-Tunes:**\\n     - Fine-tuning models with specific training data.\\n     - Capabilities include creating, listing, retrieving, canceling, and deleting fine-tune events and models.\\n   - **Image:**\\n     - Generative image AI capabilities.\\n     - Includes creating images, image edits, and variations.\\n   - **Files:**\\n     - Handling training data through file upload, retrieval, and deletion.\\n\\n4. **Use Cases:**\\n   - Some use cases for these AI services are highlighted, including:\\n     - Chats for grammar correction, code generation, writing assistance, content moderation, and product description pages.\\n\\n5. **Potential IT Tools:**\\n   - The diagram lists potential IT tools that could significantly benefit from these AI capabilities:\\n     - Design tools (Figma, Invision)\\n     - Project and dev management tools (JIRA, Rally, Trello)\\n     - Contact center AI (Asapp, Google CCAI)\\n     - IDEs (IntelliJ, Eclipse, VS)\\n     - Collaboration tools (Miro, MS Whiteboard, Google Jamboard)\\n\\nOverall, the image provides a comprehensive overview of OpenAI’s offerings within the Microsoft Azure ecosystem, detailing how different AI models and services can be utilized and integrated into various technological tools and applications.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bb624",
   "metadata": {},
   "source": [
    "# Level 3: Semantic Chunking \n",
    "\n",
    "Semantic chunking involves taking the embeddings of every sentence in the document, comparing the similarity of all sentences with each other, and then grouping sentences with the most similar embeddings together.\n",
    "By focusing on the text’s meaning and context, Semantic Chunking significantly enhances the quality of retrieval. It’s a top-notch choice when maintaining the semantic integrity of the text is vital.\n",
    "\n",
    "The hypothesis here is we can use embeddings of individual sentences to make more meaningful chunks. Basic idea is as follows :-\n",
    "\n",
    "1. Split the documents into sentences based on separators(.,?,!)\n",
    "2. Index each sentence based on position.\n",
    "3. Group: Choose how many sentences to be on either side. Add a buffer of sentences on either side of our selected sentence.\n",
    "4. Calculate distance between group of sentences.\n",
    "5. Merge groups based on similarity i.e. keep similar sentences together.\n",
    "6. Split the sentences that are not similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c608559b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#\n",
    "loader = PyPDFLoader(\"1810.04805.pdf\")\n",
    "documents = loader.load()\n",
    "#\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a32590f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT BERT \n",
      "E[CLS] E1 E[SEP] ... ENE1’... EM’\n",
      "C\n",
      "T1\n",
      "T[SEP] ...\n",
      " TN\n",
      "T1’...\n",
      " TM’\n",
      "[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \n",
      "Question Paragraph Start/End Span \n",
      "BERT \n",
      "E[CLS] E1 E[SEP] ... ENE1’... EM’\n",
      "C\n",
      "T1\n",
      "T[SEP] ...\n",
      " TN\n",
      "T1’...\n",
      " TM’\n",
      "[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \n",
      "Masked Sentence A Masked Sentence B \n",
      "Pre-training Fine-Tuning NSP Mask LM Mask LM \n",
      "Unlabeled Sentence A and B Pair SQuAD \n",
      "Question Answer Pair NER MNLI Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\n",
      "tures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\n",
      "models for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\n",
      "symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\n",
      "tions/answers).\n",
      "ing and auto-encoder objectives have been used\n",
      "for pre-training such models (Howard and Ruder,\n",
      "\n",
      "2018; Radford et al., 2018; Dai and Le, 2015).\n",
      "2.3 Transfer Learning from Supervised Data\n",
      "There has also been work showing effective trans-\n",
      "fer from supervised tasks with large datasets, such\n",
      "as natural language inference (Conneau et al.,\n",
      "2017) and machine translation (McCann et al.,\n",
      "2017). Computer vision research has also demon-\n",
      "strated the importance of transfer learning from\n",
      "large pre-trained models, where an effective recipe\n",
      "is to ﬁne-tune models pre-trained with Ima-\n",
      "geNet (Deng et al., 2009; Yosinski et al., 2014).\n",
      "3 BERT\n",
      "We introduce BERT and its detailed implementa-\n",
      "tion in this section. There are two steps in our\n",
      "framework: pre-training and ﬁne-tuning . Dur-\n",
      "ing pre-training, the model is trained on unlabeled\n",
      "data over different pre-training tasks. For ﬁne-\n",
      "tuning, the BERT model is ﬁrst initialized with\n",
      "the pre-trained parameters, and all of the param-\n",
      "eters are ﬁne-tuned using labeled data from the\n",
      "downstream tasks. Each downstream task has sep-\n",
      "\n",
      "arate ﬁne-tuned models, even though they are ini-\n",
      "tialized with the same pre-trained parameters. The\n",
      "question-answering example in Figure 1 will serve\n",
      "as a running example for this section.\n",
      "A distinctive feature of BERT is its uniﬁed ar-\n",
      "chitecture across different tasks. There is mini-mal difference between the pre-trained architec-\n",
      "ture and the ﬁnal downstream architecture.\n",
      "Model Architecture BERT’s model architec-\n",
      "ture is a multi-layer bidirectional Transformer en-\n",
      "coder based on the original implementation de-\n",
      "scribed in Vaswani et al. (2017) and released in\n",
      "thetensor2tensor library.1Because the use\n",
      "of Transformers has become common and our im-\n",
      "plementation is almost identical to the original,\n",
      "we will omit an exhaustive background descrip-\n",
      "tion of the model architecture and refer readers to\n",
      "Vaswani et al. (2017) as well as excellent guides\n",
      "such as “The Annotated Transformer.”2\n",
      "In this work, we denote the number of layers\n",
      "(i.e., Transformer blocks) as L, the hidden size as\n",
      "\n",
      "H, and the number of self-attention heads as A.3\n",
      "We primarily report results on two model sizes:\n",
      "BERT BASE (L=12, H=768, A=12, Total Param-\n",
      "eters=110M) and BERT LARGE (L=24, H=1024,\n",
      "A=16, Total Parameters=340M).\n",
      "BERT BASE was chosen to have the same model\n",
      "size as OpenAI GPT for comparison purposes.\n",
      "Critically, however, the BERT Transformer uses\n",
      "bidirectional self-attention, while the GPT Trans-\n",
      "former uses constrained self-attention where every\n",
      "token can only attend to context to its left.4\n",
      "1https://github.com/tensorﬂow/tensor2tensor\n",
      "2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
      "3In all cases we set the feed-forward/ﬁlter size to be 4H,\n",
      "i.e., 3072 for the H= 768 and 4096 for the H= 1024 .\n",
      "4We note that in the literature the bidirectional Trans-\n",
      "\n",
      "Input/Output Representations To make BERT\n",
      "handle a variety of down-stream tasks, our input\n",
      "representation is able to unambiguously represent\n",
      "both a single sentence and a pair of sentences\n",
      "(e.g.,⟨Question, Answer⟩) in one token sequence.\n",
      "Throughout this work, a “sentence” can be an arbi-\n",
      "trary span of contiguous text, rather than an actual\n",
      "linguistic sentence. A “sequence” refers to the in-\n",
      "put token sequence to BERT, which may be a sin-\n",
      "gle sentence or two sentences packed together.\n",
      "We use WordPiece embeddings (Wu et al.,\n",
      "2016) with a 30,000 token vocabulary. The ﬁrst\n",
      "token of every sequence is always a special clas-\n",
      "siﬁcation token ( [CLS] ). The ﬁnal hidden state\n",
      "corresponding to this token is used as the ag-\n",
      "gregate sequence representation for classiﬁcation\n",
      "tasks. Sentence pairs are packed together into a\n",
      "single sequence. We differentiate the sentences in\n",
      "two ways. First, we separate them with a special\n",
      "token ( [SEP] ). Second, we add a learned embed-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "#\n",
    "naive_chunks = text_splitter.split_documents(documents)\n",
    "for chunk in naive_chunks[10:15]:\n",
    "  print(chunk.page_content+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8afa6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "embed_model = OpenAIEmbeddings(openai_api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1cc39c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“LTR & No NSP” is\n",
      "trained as a left-to-right LM without the next sentence\n",
      "prediction, like OpenAI GPT. “+ BiLSTM” adds a ran-\n",
      "domly initialized BiLSTM on top of the “LTR + No\n",
      "NSP” model during ﬁne-tuning. ablation studies can be found in Appendix C. 5.1 Effect of Pre-training Tasks\n",
      "We demonstrate the importance of the deep bidi-\n",
      "rectionality of BERT by evaluating two pre-\n",
      "training objectives using exactly the same pre-\n",
      "training data, ﬁne-tuning scheme, and hyperpa-\n",
      "rameters as BERT BASE :\n",
      "No NSP : A bidirectional model which is trained\n",
      "using the “masked LM” (MLM) but without the\n",
      "“next sentence prediction” (NSP) task. LTR & No NSP : A left-context-only model which\n",
      "is trained using a standard Left-to-Right (LTR)\n",
      "LM, rather than an MLM. The left-only constraint\n",
      "was also applied at ﬁne-tuning, because removing\n",
      "it introduced a pre-train/ﬁne-tune mismatch that\n",
      "degraded downstream performance. Additionally,\n",
      "this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but\n",
      "using our larger training dataset, our input repre-\n",
      "sentation, and our ﬁne-tuning scheme. We ﬁrst examine the impact brought by the NSP\n",
      "task. In Table 5, we show that removing NSP\n",
      "hurts performance signiﬁcantly on QNLI, MNLI,\n",
      "and SQuAD 1.1. Next, we evaluate the impact\n",
      "of training bidirectional representations by com-\n",
      "paring “No NSP” to “LTR & No NSP”. The LTR\n",
      "model performs worse than the MLM model on all\n",
      "tasks, with large drops on MRPC and SQuAD. For SQuAD it is intuitively clear that a LTR\n",
      "model will perform poorly at token predictions,\n",
      "since the token-level hidden states have no right-\n",
      "side context. In order to make a good faith at-\n",
      "tempt at strengthening the LTR system, we added\n",
      "a randomly initialized BiLSTM on top. This does\n",
      "signiﬁcantly improve results on SQuAD, but theresults are still far worse than those of the pre-\n",
      "trained bidirectional models. The BiLSTM hurts\n",
      "performance on the GLUE tasks. We recognize that it would also be possible to\n",
      "train separate LTR and RTL models and represent\n",
      "each token as the concatenation of the two mod-\n",
      "els, as ELMo does. However: (a) this is twice as\n",
      "expensive as a single bidirectional model; (b) this\n",
      "is non-intuitive for tasks like QA, since the RTL\n",
      "model would not be able to condition the answer\n",
      "on the question; (c) this it is strictly less powerful\n",
      "than a deep bidirectional model, since it can use\n",
      "both left and right context at every layer. 5.2 Effect of Model Size\n",
      "In this section, we explore the effect of model size\n",
      "on ﬁne-tuning task accuracy. We trained a number\n",
      "of BERT models with a differing number of layers,\n",
      "hidden units, and attention heads, while otherwise\n",
      "using the same hyperparameters and training pro-\n",
      "cedure as described previously. Results on selected GLUE tasks are shown in\n",
      "Table 6. In this table, we report the average Dev\n",
      "Set accuracy from 5 random restarts of ﬁne-tuning. We can see that larger models lead to a strict ac-\n",
      "curacy improvement across all four datasets, even\n",
      "for MRPC which only has 3,600 labeled train-\n",
      "ing examples, and is substantially different from\n",
      "the pre-training tasks. It is also perhaps surpris-\n",
      "ing that we are able to achieve such signiﬁcant\n",
      "improvements on top of models which are al-\n",
      "ready quite large relative to the existing literature. For example, the largest Transformer explored in\n",
      "Vaswani et al. (2017) is (L=6, H=1024, A=16)\n",
      "with 100M parameters for the encoder, and the\n",
      "largest Transformer we have found in the literature\n",
      "is (L=64, H=512, A=2) with 235M parameters\n",
      "(Al-Rfou et al., 2018). By contrast, BERT BASE\n",
      "contains 110M parameters and BERT LARGE con-\n",
      "tains 340M parameters. It has long been known that increasing the\n",
      "model size will lead to continual improvements\n",
      "on large-scale tasks such as machine translation\n",
      "and language modeling, which is demonstrated\n",
      "by the LM perplexity of held-out training data\n",
      "shown in Table 6. However, we believe that\n",
      "this is the ﬁrst work to demonstrate convinc-\n",
      "ingly that scaling to extreme model sizes also\n",
      "leads to large improvements on very small scale\n",
      "tasks, provided that the model has been sufﬁ-\n",
      "ciently pre-trained. Peters et al.\n",
      "4111\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")\n",
    "#\n",
    "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
    "#\n",
    "for semantic_chunk in semantic_chunks:\n",
    "  if \"Effect of Pre-training Tasks\" in semantic_chunk.page_content:\n",
    "    print(semantic_chunk.page_content)\n",
    "    print(len(semantic_chunk.page_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
