{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52105696",
   "metadata": {},
   "source": [
    "Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d0bbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/speech.txt'}, page_content='Hi hello sample')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Ingestion\n",
    "# Text loader\n",
    "from langchain.document_loaders import TextLoader\n",
    "loader=TextLoader(\"./data/speech.txt\")\n",
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b681ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='PassengerId: 1\n",
      "Survived: 0\n",
      "Pclass: 3\n",
      "Name: Braund, Mr. Owen Harris\n",
      "Sex: male\n",
      "Age: 22\n",
      "SibSp: 1\n",
      "Parch: 0\n",
      "Ticket: A/5 21171\n",
      "Fare: 7.25\n",
      "Cabin: \n",
      "Embarked: S' metadata={'source': './data/titanic.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "#CSV Loader\n",
    "\n",
    "#The CSV loader loads a CSV file with a single row per document. The output contains both page content as well as the metadata associated.\n",
    "\n",
    "from langchain.document_loaders import CSVLoader\n",
    "loader = CSVLoader(file_path=\"./data/titanic.csv\")\n",
    "data = loader.load()\n",
    "print(data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e7c7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/openai.htm'}, page_content='ChatGPT on your desktop\\n\\nChat about email, screenshots, files, and anything on your screen.\\n\\nDownload\\n\\nAvailable now on macOS.*\\n\\nSeamlessly integrates with how you work, write, and create\\n\\nFaster access to ChatGPT\\n\\nOption + Space opens ChatGPT from any screen on your desktop.\\n\\nSpeak with ChatGPT from your desktop\\n\\nStart a conversation. Practice a new language. Tap the headphone icon to begin.\\n\\nDo more on your desktop with ChatGPT\\n\\nDownload\\n\\nThe desktop app is only available for macOS 14+ with Apple Silicon (M1 or better). Coming to Windows later this year.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HTML Loader\n",
    "\n",
    "#We can load HTML documents in a document format that we can use for further downstream tasks. We have similar syntax.\n",
    "\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "loader = UnstructuredHTMLLoader('./data/openai.htm')\n",
    "data = loader.load()\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21535e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "github-download\n",
      "\n",
      "github-download downloads commit comments and select issues metadata, saving the raw JSON and writing summary .csv files.\n",
      "\n",
      "Installing\n",
      "\n",
      "Download the .jar file here. It includes all dependencies. You must have the Java Runtime Environment version 7 or above.\n",
      "\n",
      "Usage\n",
      "\n",
      "github-download can be run from the command line. It has three required flags:\n",
      "\n",
      "repo. The full repository name, e.g., PovertyAction/github-download.\n",
      "\n",
      "to. The directory in which to save the metadata. It will be created if it does not exist already.\n",
      "\n",
      "token. The name of a text file that contains solely a GitHub OAuth token. GitHub will supply you a token, which is a single string. You must copy it to a text file, then specify the name of that file to -token.\n",
      "\n",
      "All together:\n",
      "\n",
      "java -jar github-download.jar -repo PovertyAction/github-download -token token.txt -to metadata\n",
      "\n",
      "If the name of the .jar file is not github-download.jar, use the actual filename in the command above, or rename the file as github-download.jar. If the file is not in the current working directory, you will have to specify its path.\n",
      "\n",
      "Next, specify the metadata to download:\n",
      "\n",
      "issues. Download select issues metadata.\n",
      "\n",
      "cc. Download commit comments, including in-line notes.\n",
      "\n",
      "To download all supported metadata:\n",
      "\n",
      "java -jar github-download.jar -repo PovertyAction/github-download -token token.txt -to metadata -issues -cc\n",
      "\n",
      "You may see the following warning message, which is safe to ignore:\n",
      "\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "#Markdown Loader\n",
    "\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "loader = UnstructuredMarkdownLoader(file_path='./data/README.md')\n",
    "data = loader.load()\n",
    "print(data[0].page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf545fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.0 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature.\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\n",
      "in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [31, 21, 13].\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n"
     ]
    }
   ],
   "source": [
    "# PDF Loader\n",
    "\n",
    "#Here we use PyPDF load the PDF documents. Each document contains the page content and metadata with page numbers.\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('./data/attention.pdf')\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0].page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b089789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LangChain',\n",
       " 'summary': \"LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\",\n",
       " 'source': 'https://en.wikipedia.org/wiki/LangChain'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Wikipedia Loader\n",
    "\n",
    "#We use WikipediaLoader to fetch the documents. It mainly has 3 arguments: query: which is used to find the topic in Wikipedia, optional lang: default=“en”, and load_max_docs: default=100, which is used to limit the number of documents downloaded.\n",
    "\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "loader = WikipediaLoader(query='LangChain', load_max_docs=1)\n",
    "data = loader.load()\n",
    "data[0].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a96a6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2023-08-02',\n",
       " 'Title': 'Attention Is All You Need',\n",
       " 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
       " 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ArXiv Loader\n",
    "\n",
    "#arXiv is an open-access archive for 2 million scholarly articles. We can use ArxivLoader to extract information of any paper. We need the article id that would be available in the URL of the paper to use the loader.\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "loader = ArxivLoader(query='1706.03762', load_max_docs=1) \n",
    "data = loader.load()\n",
    "data[0].metadata\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
